{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3lfaHNAlskK"
   },
   "source": [
    "### 사전환경 설정 및 github에서 styleGAN2-ada모델 가져오는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OfdL1GRnk9AS"
   },
   "outputs": [],
   "source": [
    "# 코랩과 구글드라이브를 연동해서 사용하려면 주석을 풀고 이 셀을 실행시키면 됩니다.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 명령어는 코랩에서만 작동됩니다. 주피터 노트북의 경우 tensorflow 버전을 다른 명령어로 사용하여 바꿔줘야합니다. \n",
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rTnk3l1ckyw1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\r\n"
     ]
    }
   ],
   "source": [
    "# 그래픽카드의 정보를 확인할 수 있는 명령어 입니다. 이 명령어 또한 코랩에서만 작동합니다.\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CwzGeMrvlrcz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/parkheondo/Desktop/stylebot_project\n",
      "/Users/parkheondo/Desktop/stylebot_project/colab-sg2-ada\n",
      "'stylegan2-ada'에 복제합니다...\n",
      "remote: Enumerating objects: 71, done.\u001b[K\n",
      "remote: Total 71 (delta 0), reused 0 (delta 0), pack-reused 71\u001b[K\n",
      "오브젝트를 받는 중: 100% (71/71), 1.28 MiB | 11.04 MiB/s, 완료.\n",
      "델타를 알아내는 중: 100% (22/22), 완료.\n",
      "/Users/parkheondo/Desktop/stylebot_project/colab-sg2-ada/stylegan2-ada\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.path.isdir(\"/content/drive/My Drive/colab-sg2-ada\"):\n",
    "    %cd \"/content/drive/My Drive/colab-sg2-ada/stylegan2-ada\"\n",
    "else:\n",
    "    #install script\n",
    "    %cd \"/content/drive/My Drive/\"\n",
    "    !mkdir colab-sg2-ada\n",
    "    %cd colab-sg2-ada\n",
    "    !git clone https://github.com/NVlabs/stylegan2-ada\n",
    "    %cd stylegan2-ada\n",
    "    !mkdir downloads\n",
    "    !mkdir datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zaAi8Jlkmkb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/parkheondo/Desktop/stylebot_project/colab-sg2-ada/stylegan2-ada\n"
     ]
    }
   ],
   "source": [
    "%cd \"/content/drive/My Drive/colab-sg2-ada/stylegan2-ada\"\n",
    "!git config --global user.name \"test\"\n",
    "!git config --global user.email \"test@test.com\"\n",
    "!git fetch origin\n",
    "!git checkout origin/main -- train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGcEXYzAmO_z"
   },
   "source": [
    "### 데이터 전처리 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJpSpc6oal2k"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "import os\n",
    "import PIL.Image\n",
    "\n",
    "\n",
    "def image_align(src_file, dst_file, face_landmarks, output_size=256, transform_size=4096, enable_padding=True):\n",
    "        # Align function from FFHQ dataset pre-processing step\n",
    "        # https://github.com/NVlabs/ffhq-dataset/blob/master/download_ffhq.py\n",
    "\n",
    "        lm = np.array(face_landmarks)\n",
    "        lm_chin          = lm[0  : 17]  # left-right\n",
    "        lm_eyebrow_left  = lm[17 : 22]  # left-right\n",
    "        lm_eyebrow_right = lm[22 : 27]  # left-right\n",
    "        lm_nose          = lm[27 : 31]  # top-down\n",
    "        lm_nostrils      = lm[31 : 36]  # top-down\n",
    "        lm_eye_left      = lm[36 : 42]  # left-clockwise\n",
    "        lm_eye_right     = lm[42 : 48]  # left-clockwise\n",
    "        lm_mouth_outer   = lm[48 : 60]  # left-clockwise\n",
    "        lm_mouth_inner   = lm[60 : 68]  # left-clockwise\n",
    "\n",
    "        # Calculate auxiliary vectors.\n",
    "        eye_left     = np.mean(lm_eye_left, axis=0)\n",
    "        eye_right    = np.mean(lm_eye_right, axis=0)\n",
    "        eye_avg      = (eye_left + eye_right) * 0.5\n",
    "        eye_to_eye   = eye_right - eye_left\n",
    "        mouth_left   = lm_mouth_outer[0]\n",
    "        mouth_right  = lm_mouth_outer[6]\n",
    "        mouth_avg    = (mouth_left + mouth_right) * 0.5\n",
    "        eye_to_mouth = mouth_avg - eye_avg\n",
    "\n",
    "        # Choose oriented crop rectangle.\n",
    "        x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n",
    "        x /= np.hypot(*x)\n",
    "        x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n",
    "        y = np.flipud(x) * [-1, 1]\n",
    "        c = eye_avg + eye_to_mouth * 0.1\n",
    "        quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n",
    "        qsize = np.hypot(*x) * 2\n",
    "\n",
    "        # Load in-the-wild image.\n",
    "        if not os.path.isfile(src_file):\n",
    "            print('\\nCannot find source image. Please run \"--wilds\" before \"--align\".')\n",
    "            return\n",
    "        img = PIL.Image.open(src_file)\n",
    "\n",
    "        # Shrink.\n",
    "        shrink = int(np.floor(qsize / output_size * 0.5))\n",
    "        if shrink > 1:\n",
    "            rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n",
    "            img = img.resize(rsize, PIL.Image.ANTIALIAS)\n",
    "            quad /= shrink\n",
    "            qsize /= shrink\n",
    "\n",
    "        # Crop.\n",
    "        border = max(int(np.rint(qsize * 0.1)), 3)\n",
    "        crop = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
    "        crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]), min(crop[3] + border, img.size[1]))\n",
    "        if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n",
    "            img = img.crop(crop)\n",
    "            quad -= crop[0:2]\n",
    "\n",
    "        # Pad.\n",
    "        pad = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
    "        pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0), max(pad[3] - img.size[1] + border, 0))\n",
    "        if enable_padding and max(pad) > border - 4:\n",
    "            pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n",
    "            img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n",
    "            h, w, _ = img.shape\n",
    "            y, x, _ = np.ogrid[:h, :w, :1]\n",
    "            mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w-1-x) / pad[2]), 1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h-1-y) / pad[3]))\n",
    "            blur = qsize * 0.02\n",
    "            img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n",
    "            img += (np.median(img, axis=(0,1)) - img) * np.clip(mask, 0.0, 1.0)\n",
    "            img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)), 'RGB')\n",
    "            quad += pad[:2]\n",
    "\n",
    "        # Transform.\n",
    "        img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)\n",
    "        if output_size < transform_size:\n",
    "            img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)\n",
    "\n",
    "        # Save aligned image.\n",
    "        img.save(dst_file, 'PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkjfJ12ebCPL"
   },
   "outputs": [],
   "source": [
    "import dlib\n",
    "\n",
    "\n",
    "class LandmarksDetector:\n",
    "    def __init__(self, predictor_model_path):\n",
    "        \"\"\"\n",
    "        :param predictor_model_path: path to shape_predictor_68_face_landmarks.dat file\n",
    "        \"\"\"\n",
    "        self.detector = dlib.get_frontal_face_detector() # cnn_face_detection_model_v1 also can be used\n",
    "        self.shape_predictor = dlib.shape_predictor(predictor_model_path)\n",
    "\n",
    "    def get_landmarks(self, image):\n",
    "        img = dlib.load_rgb_image(image)\n",
    "        dets = self.detector(img, 1)\n",
    "\n",
    "        for detection in dets:\n",
    "            face_landmarks = [(item.x, item.y) for item in self.shape_predictor(img, detection).parts()]\n",
    "            yield face_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUSfWlZKavKM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import bz2\n",
    "import tensorflow as tf\n",
    "\n",
    "LANDMARKS_MODEL_URL = 'http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2'\n",
    "\n",
    "\n",
    "def unpack_bz2(src_path):\n",
    "    data = bz2.BZ2File(src_path).read()\n",
    "    dst_path = src_path[:-4]\n",
    "    with open(dst_path, 'wb') as fp:\n",
    "        fp.write(data)\n",
    "    return dst_path\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Extracts and aligns all faces from images using DLib and a function from original FFHQ dataset preparation step\n",
    "    python align_images.py /raw_images /aligned_images\n",
    "    \"\"\"\n",
    "\n",
    "    landmarks_model_path = unpack_bz2(tf.keras.utils.get_file('shape_predictor_68_face_landmarks.dat.bz2',\n",
    "                                               LANDMARKS_MODEL_URL, cache_subdir='temp'))\n",
    "    \n",
    "    RAW_IMAGES_DIR = '전처리 전 데이터의 파일경로' # 전처리 전 데이터의 파일경로\n",
    "    ALIGNED_IMAGES_DIR = '# 전처리 후 데이터를 받을 파일경로' # 전처리 후 데이터를 받을 파일경로\n",
    "\n",
    "    landmarks_detector = LandmarksDetector(landmarks_model_path)\n",
    "    for img_name in [f for f in os.listdir(RAW_IMAGES_DIR) if f[0] not in '._']:\n",
    "        raw_img_path = os.path.join(RAW_IMAGES_DIR, img_name)\n",
    "        for i, face_landmarks in enumerate(landmarks_detector.get_landmarks(raw_img_path), start=1):\n",
    "            face_img_name = '%s_%02d.jpg' % (os.path.splitext(img_name)[0], i)\n",
    "            aligned_face_path = os.path.join(ALIGNED_IMAGES_DIR, face_img_name)\n",
    "            os.makedirs(ALIGNED_IMAGES_DIR, exist_ok=True)\n",
    "            image_align(raw_img_path, aligned_face_path, face_landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wsvr4GBamXMx"
   },
   "source": [
    "### 데이터셋 만드는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cgsr2cgRmcdg"
   },
   "source": [
    "styleGAN2에 원하는 데이터를 학습시키기전에 일단 데이터셋으로 만들어주어야합니다. styleGAN2에 데이터셋을 만드는 기능이 있어서 바로 활용했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8r2lsGuzuNI0"
   },
   "outputs": [],
   "source": [
    "#update this to the path to your image folder\n",
    "dataset_path = \"데이터셋의 경로를 넣어주세요\"\n",
    "#give your dataset a name\n",
    "dataset_name = \"데이터셋의 이름을 입력해주세요\"\n",
    "\n",
    "#you don't need to edit anything here\n",
    "!python dataset_tool.py create_from_images /content/drive/MyDrive/colab-sg2-ada/stylegan2-ada/datasets/{dataset_name} {dataset_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53HRXMY-m1Ti"
   },
   "source": [
    "### 데이터 학습 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZj9N02fm5uN"
   },
   "source": [
    "첫번째 셀을 실행시키면 train.py를 실행시키는데 parameter를 볼 수 있습니다. 필수로 필요한 파라미터는 outdir, data가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mkHv1tNWne31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"train.py\", line 18, in <module>\r\n",
      "    import dnnlib.tflib as tflib\r\n",
      "  File \"/Users/parkheondo/Desktop/stylebot_project/colab-sg2-ada/stylegan2-ada/dnnlib/tflib/__init__.py\", line 9, in <module>\r\n",
      "    from . import autosummary\r\n",
      "  File \"/Users/parkheondo/Desktop/stylebot_project/colab-sg2-ada/stylegan2-ada/dnnlib/tflib/autosummary.py\", line 32, in <module>\r\n",
      "    from . import tfutil\r\n",
      "  File \"/Users/parkheondo/Desktop/stylebot_project/colab-sg2-ada/stylegan2-ada/dnnlib/tflib/tfutil.py\", line 18, in <module>\r\n",
      "    import tensorflow.contrib   # requires TensorFlow 1.x!\r\n",
      "ModuleNotFoundError: No module named 'tensorflow.contrib'\r\n"
     ]
    }
   ],
   "source": [
    "!python train.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0qmEr3bVU2X"
   },
   "outputs": [],
   "source": [
    "!python train.py \\\n",
    " --outdir='/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada/results' \\\n",
    " --snap=4 \\\n",
    " --data='데이터셋의 경로를 입력해주세요' \\\n",
    " --mirror=true \\\n",
    " --metrics=none \\\n",
    " --cfg=auto \\\n",
    " --resume='사전학습모델의 경로를 입력해주세요' \\\n",
    " --kimg=300 \\\n",
    " --freezed=10 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuCg2JVYoCHK"
   },
   "source": [
    "--outdir에는 학습결과를 받을 파일경로를 입력해줍니다.\n",
    "\n",
    "--data에는 dataset이 들어있는 경로를 입력해줍니다.\n",
    "\n",
    "--resume의 경우 사전학습모델이 있다면 입력해줍니다.\n",
    "\n",
    "--kimg의 경우는 학습시킬 이미지 수를 입력해줍니다.\n",
    "\n",
    "--freezed는 판별자를 얼마나 얼릴지 지정해주는 변수입니다. 과적합이 일어나면 판별자층을 어느정도 얼려주어서 기존에 학습한 가중치를 최대한 활용하는 것이 <br>좋습니다.\n",
    "\n",
    "--snap의 경우 snapshot을 어느주기로 남겨줄지 정해주는 겁니다. 예를 들어 4로 설정해두면 이미지 4개를 학습할 때 마다 학습모델과 결과를 파일로 남겨줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPXuQo19pPHa"
   },
   "source": [
    "### Image generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MuxbAHA5yf_W"
   },
   "outputs": [],
   "source": [
    "pip install opensimplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIAtFq_QoLRI"
   },
   "outputs": [],
   "source": [
    "!python generate.py generate-images --outdir=out2 --trunc=1 --seeds=401-800 \\\n",
    "      --network='./results/00048-short-mirror-auto1-kimg300-resumecustom-freezed10/network-snapshot-000300.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWxd4Xn_p4Bp"
   },
   "source": [
    "generate를 실행하려면 opensimplex를 설치해주어야합니다. \n",
    "\n",
    "--network는 어떤 모델로 훈련시킬 지 지정해주는 변수입니다. 모델이 있는 파일경로를 넣어주면 됩니다.\n",
    "\n",
    "--outdir는 생성된 이미지를 어디로 받을 지 경로를 지정해줍니다.\n",
    "\n",
    "--seeds는 시드를 지정해주는 변수입니다. 랜덤으로 넣어주셔도 되고 지정해서 넣어주셔도 됩니다."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Untitled9 (1).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
